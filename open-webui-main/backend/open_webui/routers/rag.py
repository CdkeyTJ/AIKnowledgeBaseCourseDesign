from fastapi import APIRouter, Depends, HTTPException, Request, UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Annotated, Optional
import os
import uuid
import json
import pickle
from datetime import datetime
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from open_webui.utils.auth import get_current_user
from open_webui.models.users import UserModel

router = APIRouter()

# æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
STORAGE_DIR = "rag_storage"
METADATA_FILE = "metadata.json"
VECTOR_DIR = "vectors"

# ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
os.makedirs(STORAGE_DIR, exist_ok=True)
os.makedirs(os.path.join(STORAGE_DIR, VECTOR_DIR), exist_ok=True)

# å…¨å±€å­˜å‚¨ - å®é™…ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æ•°æ®åº“
user_kbs: Dict[str, Dict[str, Dict]] = {}  # user_id -> kb_id -> kb_info
vector_stores: Dict[str, FAISS] = {}  # vector_key -> FAISSå®ä¾‹

# åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # å‡å°åˆ†å—å¤§å°ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
    chunk_overlap=100,  # å‡å°é‡å ï¼Œé¿å…é‡å¤
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
)

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2",
    model_kwargs={'device': 'cpu'},  # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
    encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥å‘é‡
)

# æ•°æ®æ¨¡å‹
class KnowledgeBaseCreate(BaseModel):
    name: str
    description: Optional[str] = ""

class QueryRequest(BaseModel):
    question: str
    top_k: int = 3

# æŒä¹…åŒ–å­˜å‚¨å‡½æ•°
def save_metadata():
    """ä¿å­˜çŸ¥è¯†åº“å…ƒæ•°æ®åˆ°æ–‡ä»¶"""
    metadata_path = os.path.join(STORAGE_DIR, METADATA_FILE)
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(user_kbs, f, ensure_ascii=False, indent=2, default=str)

def load_metadata():
    """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“å…ƒæ•°æ®"""
    metadata_path = os.path.join(STORAGE_DIR, METADATA_FILE)
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # è½¬æ¢æ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡
                for user_id, kbs in data.items():
                    for kb_id, kb_info in kbs.items():
                        if 'created_at' in kb_info:
                            kb_info['created_at'] = datetime.fromisoformat(kb_info['created_at'])
                return data
        except Exception as e:
            print(f"åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
    return {}

def save_vector_store(vector_key: str, vector_store: FAISS):
    """ä¿å­˜å‘é‡å­˜å‚¨åˆ°æ–‡ä»¶"""
    vector_path = os.path.join(STORAGE_DIR, VECTOR_DIR, f"{vector_key}.pkl")
    try:
        with open(vector_path, 'wb') as f:
            pickle.dump(vector_store, f)
    except Exception as e:
        print(f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")

def load_vector_store(vector_key: str) -> Optional[FAISS]:
    """ä»æ–‡ä»¶åŠ è½½å‘é‡å­˜å‚¨"""
    vector_path = os.path.join(STORAGE_DIR, VECTOR_DIR, f"{vector_key}.pkl")
    if os.path.exists(vector_path):
        try:
            with open(vector_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
    return None

# å¯åŠ¨æ—¶åŠ è½½æ•°æ®
def initialize_storage():
    """åˆå§‹åŒ–å­˜å‚¨ï¼ŒåŠ è½½å·²ä¿å­˜çš„æ•°æ®"""
    global user_kbs, vector_stores
    
    # åŠ è½½å…ƒæ•°æ®
    user_kbs = load_metadata()
    
    # åŠ è½½å‘é‡å­˜å‚¨
    for user_id, kbs in user_kbs.items():
        for kb_id in kbs.keys():
            vector_key = f"{user_id}_{kb_id}"
            vector_store = load_vector_store(vector_key)
            if vector_store:
                vector_stores[vector_key] = vector_store
                print(f"å·²åŠ è½½å‘é‡å­˜å‚¨: {vector_key}")
            else:
                # å¦‚æœæ²¡æœ‰ä¿å­˜çš„å‘é‡å­˜å‚¨ï¼Œè®¾ç½®ä¸ºNoneï¼ˆç­‰å¾…æ–‡æ¡£ä¸Šä¼ ï¼‰
                vector_stores[vector_key] = None
                print(f"å‘é‡å­˜å‚¨ä¸ºç©º: {vector_key}ï¼Œç­‰å¾…æ–‡æ¡£ä¸Šä¼ ")

# åœ¨æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
initialize_storage()

# çŸ¥è¯†åº“ç®¡ç†æ¥å£
@router.post("/knowledge-bases", response_model=Dict)
async def create_knowledge_base(
    request: Request,
    kb_data: KnowledgeBaseCreate,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    kb_id = str(uuid.uuid4())
    kb_name = kb_data.name
    
    # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
    base_dir = os.path.join("knowledge_bases", user_id)
    os.makedirs(base_dir, exist_ok=True)
    kb_dir = os.path.join(base_dir, kb_id)
    os.makedirs(kb_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨
    vector_key = f"{user_id}_{kb_id}"
    # ä¸è¦ç”¨ç©ºå­—ç¬¦ä¸²åˆå§‹åŒ–ï¼Œè¿™ä¼šå¯¼è‡´é—®é¢˜
    # vector_stores[vector_key] = FAISS.from_texts([""], embeddings)
    # åˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨ï¼Œç­‰å¾…æ–‡æ¡£ä¸Šä¼ 
    vector_stores[vector_key] = None
    
    # è®°å½•çŸ¥è¯†åº“ä¿¡æ¯
    if user_id not in user_kbs:
        user_kbs[user_id] = {}
    
    user_kbs[user_id][kb_id] = {
        "id": kb_id,
        "name": kb_name,
        "description": kb_data.description,
        "directory": kb_dir,
        "created_at": datetime.now().isoformat(),
        "documents": []
    }
    
    # @CDK: ä¿å­˜å…ƒæ•°æ®åˆ°æ–‡ä»¶
    save_metadata()
    
    return {
        "status": "success",
        "data": user_kbs[user_id][kb_id]
    }

@router.get("/knowledge-bases", response_model=Dict)
async def get_knowledge_bases(
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    return {
        "status": "success",
        "data": list(user_kbs.get(user_id, {}).values())
    }

@router.get("/knowledge-bases/{kb_id}", response_model=Dict)
async def get_knowledge_base(
    kb_id: str,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    return {
        "status": "success",
        "data": user_kbs[user_id][kb_id]
    }

# @CDK: æ·»åŠ çŸ¥è¯†åº“åˆ é™¤åŠŸèƒ½
@router.delete("/knowledge-bases/{kb_id}", response_model=Dict)
async def delete_knowledge_base(
    kb_id: str,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    kb_info = user_kbs[user_id][kb_id]
    vector_key = f"{user_id}_{kb_id}"
    
    try:
        # åˆ é™¤å‘é‡å­˜å‚¨æ–‡ä»¶
        vector_path = os.path.join(STORAGE_DIR, VECTOR_DIR, f"{vector_key}.pkl")
        if os.path.exists(vector_path):
            os.remove(vector_path)
            print(f"å·²åˆ é™¤å‘é‡å­˜å‚¨æ–‡ä»¶: {vector_path}")
        
        # åˆ é™¤çŸ¥è¯†åº“ç›®å½•
        if os.path.exists(kb_info["directory"]):
            import shutil
            shutil.rmtree(kb_info["directory"])
            print(f"å·²åˆ é™¤çŸ¥è¯†åº“ç›®å½•: {kb_info['directory']}")
        
        # ä»å†…å­˜ä¸­ç§»é™¤
        if vector_key in vector_stores:
            del vector_stores[vector_key]
        del user_kbs[user_id][kb_id]
        
        # å¦‚æœç”¨æˆ·æ²¡æœ‰å…¶ä»–çŸ¥è¯†åº“ï¼Œæ¸…ç†ç”¨æˆ·è®°å½•
        if not user_kbs[user_id]:
            del user_kbs[user_id]
        
        # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
        save_metadata()
        
        return {
            "status": "success",
            "message": "çŸ¥è¯†åº“å·²åˆ é™¤"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {str(e)}")

# æ–‡æ¡£å¤„ç†æ¥å£
@router.post("/knowledge-bases/{kb_id}/documents", response_model=Dict)
async def upload_document(
    current_user: Annotated[UserModel, Depends(get_current_user)],
    kb_id: str,
    file: UploadFile = File(...),
):
    user_id = current_user.id
    vector_key = f"{user_id}_{kb_id}"
    
    # éªŒè¯çŸ¥è¯†åº“å­˜åœ¨
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    kb_info = user_kbs[user_id][kb_id]
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if file_ext not in ['.pdf', '.txt']:
        raise HTTPException(status_code=400, detail="ä»…æ”¯æŒPDFå’ŒTXTæ–‡ä»¶")
    
    # ä¿å­˜æ–‡ä»¶
    file_id = str(uuid.uuid4())
    file_path = os.path.join(kb_info["directory"], f"{file_id}{file_ext}")
    
    try:
        # ä¿å­˜æ–‡ä»¶å†…å®¹
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
            file_size = len(content)
        
        # åŠ è½½æ–‡æ¡£
        if file_ext == '.pdf':
            loader = PyPDFLoader(file_path)
            documents = loader.load()
        else:
            loader = TextLoader(file_path, encoding='utf-8')
            documents = loader.load()
        
        # æ–‡æ¡£åˆ†å‰²
        split_docs = text_splitter.split_documents(documents)
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        if vector_stores[vector_key] is None:
            # ç¬¬ä¸€æ¬¡ä¸Šä¼ æ–‡æ¡£ï¼Œåˆ›å»ºå‘é‡å­˜å‚¨
            print(f"ğŸ†• é¦–æ¬¡åˆ›å»ºå‘é‡å­˜å‚¨: {vector_key}")
            vector_stores[vector_key] = FAISS.from_documents(split_docs, embeddings)
        else:
            # å‘ç°æœ‰å‘é‡å­˜å‚¨æ·»åŠ æ–‡æ¡£
            print(f"â• å‘ç°æœ‰å‘é‡å­˜å‚¨æ·»åŠ æ–‡æ¡£: {vector_key}")
            vector_stores[vector_key].add_documents(split_docs)

        # è®°å½•æ–‡æ¡£ä¿¡æ¯
        doc_info = {
            "id": file_id,
            "filename": file.filename,
            "file_type": file_ext[1:],
            "size": file_size,
            "uploaded_at": datetime.now().isoformat(),
            "chunks": len(split_docs)
        }
        kb_info["documents"].append(doc_info)
        
        # @CDK: ä¿å­˜å…ƒæ•°æ®å’Œå‘é‡å­˜å‚¨åˆ°æ–‡ä»¶
        save_metadata()
        save_vector_store(vector_key, vector_stores[vector_key])
        
        return {
            "status": "success",
            "data": doc_info
        }
        
    except Exception as e:
        if os.path.exists(file_path):
            os.remove(file_path)
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    finally:
        await file.close()

@router.get("/knowledge-bases/{kb_id}/documents", response_model=Dict)
async def get_documents(
    kb_id: str,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    return {
        "status": "success",
        "data": user_kbs[user_id][kb_id]["documents"]
    }

@router.delete("/knowledge-bases/{kb_id}/documents/{doc_id}", response_model=Dict)
async def delete_document(
    kb_id: str,
    doc_id: str,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    kb_info = user_kbs[user_id][kb_id]
    doc_index = next((i for i, doc in enumerate(kb_info["documents"]) if doc["id"] == doc_id), None)
    
    if doc_index is None:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    # åˆ é™¤æ–‡ä»¶
    doc = kb_info["documents"][doc_index]
    file_path = os.path.join(kb_info["directory"], f"{doc['id']}.{doc['file_type']}")
    if os.path.exists(file_path):
        os.remove(file_path)
    
    # ä»åˆ—è¡¨ä¸­ç§»é™¤
    del kb_info["documents"][doc_index]
    
    return {
        "status": "success",
        "message": "æ–‡æ¡£å·²åˆ é™¤"
    }

# RAGæ£€ç´¢æ¥å£
@router.post("/knowledge-bases/{kb_id}/query", response_model=Dict)
async def query_knowledge_base(
    kb_id: str,
    request: QueryRequest,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    user_id = current_user.id
    vector_key = f"{user_id}_{kb_id}"
    
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    if vector_key not in vector_stores:
        raise HTTPException(status_code=500, detail="å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
    
    try:
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = vector_stores[vector_key].similarity_search(
            request.question, 
            k=request.top_k
        )
        
        # æå–å†…å®¹
        context = "\n\n".join([doc.page_content for doc in docs])
        
        return {
            "status": "success",
            "data": {
                "context": context,
                "sources": [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in docs]
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ£€ç´¢å¤±è´¥: {str(e)}")

# ç®€å•çš„å‘é‡å­˜å‚¨çŠ¶æ€æ£€æŸ¥
@router.get("/knowledge-bases/{kb_id}/status", response_model=Dict)
async def check_kb_status(
    kb_id: str,
    current_user: Annotated[UserModel, Depends(get_current_user)]
):
    """æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€"""
    user_id = current_user.id
    vector_key = f"{user_id}_{kb_id}"
    
    if user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")
    
    vector_store = vector_stores.get(vector_key)
    
    if vector_store is None:
        return {"status": "empty", "message": "å‘é‡å­˜å‚¨ä¸ºç©º"}
    
    # æ£€æŸ¥æ–‡æ¡£æ•°é‡
    doc_count = 0
    if hasattr(vector_store, 'docstore') and hasattr(vector_store.docstore, '_dict'):
        doc_count = len(vector_store.docstore._dict)
    
    return {
        "status": "active", 
        "doc_count": doc_count,
        "vector_key": vector_key
    }

# ä¾›å†…éƒ¨è°ƒç”¨çš„æ£€ç´¢å‡½æ•°ï¼ˆç»™question_generatorä½¿ç”¨ï¼‰
def retrieve_knowledge(kb_id: str, question: str, top_k: int = 3, user_id: str = None) -> str:
    print(f"\n=== RAGæ£€ç´¢è°ƒè¯•ä¿¡æ¯ ===")
    print(f"ç”¨æˆ·ID: {user_id}")
    print(f"çŸ¥è¯†åº“ID: {kb_id}")
    print(f"æ£€ç´¢é—®é¢˜: {question}")
    print(f"æ£€ç´¢æ•°é‡: {top_k}")
    
    # ä¸¥æ ¼ æ–°å¢ç”¨æˆ·è®¤è¯æ ¡éªŒ
    if not user_id or user_id not in user_kbs or kb_id not in user_kbs[user_id]:
        print("âŒ ç¼ºå°‘æˆæƒæˆ–çŸ¥è¯†åº“ä¸å­˜åœ¨")
        print(f"user_idå­˜åœ¨: {bool(user_id)}")
        print(f"user_idåœ¨user_kbsä¸­: {user_id in user_kbs if user_id else False}")
        if user_id and user_id in user_kbs:
            print(f"è¯¥ç”¨æˆ·çš„çŸ¥è¯†åº“: {list(user_kbs[user_id].keys())}")
        return ""  # æ— æƒé™æˆ–çŸ¥è¯†åº“ä¸å­˜åœ¨æ—¶è¿”å›ç©º
    
    vector_key = f"{user_id}_{kb_id}"
    print(f"å‘é‡å­˜å‚¨é”®: {vector_key}")
    
    if vector_key not in vector_stores:
        print(f"âŒ å‘é‡å­˜å‚¨æœªæ‰¾åˆ°: {vector_key}")
        print(f"å¯ç”¨çš„å‘é‡å­˜å‚¨é”®: {list(vector_stores.keys())}")
        return ""
    
    # æ£€æŸ¥å‘é‡å­˜å‚¨çŠ¶æ€
    vector_store = vector_stores[vector_key]
    print(f"å‘é‡å­˜å‚¨ç±»å‹: {type(vector_store)}")
    
    # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦æœ‰æ•ˆ
    if vector_store is None:
        print("âŒ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        return ""
    
    # æ£€æŸ¥å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ•°é‡
    try:
        # å°è¯•è·å–å‘é‡å­˜å‚¨çš„åŸºæœ¬ä¿¡æ¯
        if hasattr(vector_store, 'index_to_docstore_id'):
            doc_count = len(vector_store.index_to_docstore_id)
            print(f"å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£æ•°é‡: {doc_count}")
        else:
            print("âš ï¸ æ— æ³•è·å–å‘é‡å­˜å‚¨æ–‡æ¡£æ•°é‡")
            doc_count = "æœªçŸ¥"
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦ä¸ºç©º
        if hasattr(vector_store, 'docstore') and hasattr(vector_store.docstore, '_dict'):
            actual_docs = len(vector_store.docstore._dict)
            print(f"å®é™…å­˜å‚¨çš„æ–‡æ¡£æ•°é‡: {actual_docs}")
            
            if actual_docs == 0:
                print("âŒ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œæ²¡æœ‰æ–‡æ¡£è¢«ç´¢å¼•")
                return ""
        else:
            print("âš ï¸ æ— æ³•æ£€æŸ¥å‘é‡å­˜å‚¨å†…å®¹")
            
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥å‘é‡å­˜å‚¨çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    # @CDK: ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ - æå–ä¸“ä¸šå…³é”®è¯
    def extract_keywords(text):
        """ä»é—®é¢˜ä¸­æå–ä¸“ä¸šå…³é”®è¯ï¼Œç®€åŒ–é€»è¾‘ç¡®ä¿èƒ½æå–åˆ°å†…å®¹"""
        print(f"ğŸ” å…³é”®è¯æå–è°ƒè¯•:")
        print(f"  åŸå§‹æ–‡æœ¬: '{text}'")
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼šç§»é™¤æ ‡ç‚¹ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„è¯
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡å’Œè‹±æ–‡
        cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        words = cleaned_text.split()
        
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯å’Œçº¯æ•°å­—ï¼Œä½†ä¿ç•™æ›´å¤šè¯æ±‡
        keywords = [word for word in words if len(word) > 1 and not word.isdigit()]
        
        print(f"  æå–çš„å…³é”®è¯: {keywords}")
        
        if keywords:
            result = ' '.join(keywords)
            print(f"  âœ… æœ€ç»ˆå…³é”®è¯: '{result}'")
            return result
        else:
            print(f"  âš ï¸ æ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸå†…å®¹")
            return text
    
    try:
        # æå–ä¸“ä¸šå…³é”®è¯
        search_query = extract_keywords(question)
        print(f"åŸå§‹é—®é¢˜: {question}")
        print(f"æå–çš„å…³é”®è¯: {search_query}")
        
        # ä½¿ç”¨å…³é”®è¯è¿›è¡Œæ£€ç´¢ - é™ä½é˜ˆå€¼ï¼Œç¡®ä¿èƒ½æ£€ç´¢åˆ°å†…å®¹
        print(f"ğŸ” å¼€å§‹å…³é”®è¯æ£€ç´¢: '{search_query}'")
        
        # å…ˆå°è¯•æ— é˜ˆå€¼æ£€ç´¢ï¼Œç¡®ä¿èƒ½è¿”å›ç»“æœ
        docs = vector_store.similarity_search_with_score(
            search_query, 
            k=top_k
        )
        
        print(f"å…³é”®è¯æ£€ç´¢ç»“æœæ•°é‡: {len(docs)}")
        
        if docs:
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å’Œç›¸ä¼¼åº¦åˆ†æ•°
            print("ğŸ“„ æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç‰‡æ®µ:")
            for i, (doc, score) in enumerate(docs):
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                print(f"  æ–‡æ¡£{i+1} (ç›¸ä¼¼åº¦: {score:.3f}): {content_preview}")
            
            # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œä¸è¿‡æ»¤ç›¸ä¼¼åº¦
            content = "\n\n".join([doc.page_content for doc, _ in docs])
            print(f"âœ… æ£€ç´¢æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
            return content
        else:
            print("âŒ å…³é”®è¯æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
        # å¦‚æœå…³é”®è¯æ£€ç´¢å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸé—®é¢˜æ£€ç´¢
        print(f"ğŸ”„ å°è¯•ä½¿ç”¨åŸé—®é¢˜æ£€ç´¢: '{question}'")
        docs_with_score = vector_store.similarity_search_with_score(
            question, 
            k=top_k
        )
        
        print(f"åŸé—®é¢˜æ£€ç´¢ç»“æœæ•°é‡: {len(docs_with_score)}")
        
        if docs_with_score:
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å’Œç›¸ä¼¼åº¦åˆ†æ•°
            print("ğŸ“„ åŸé—®é¢˜æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç‰‡æ®µ:")
            for i, (doc, score) in enumerate(docs_with_score):
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                print(f"  æ–‡æ¡£{i+1} (ç›¸ä¼¼åº¦: {score:.3f}): {content_preview}")
            
            # è¿”å›æ‰€æœ‰æ£€ç´¢ç»“æœ
            content = "\n\n".join([doc.page_content for doc, _ in docs_with_score])
            print(f"âœ… åŸé—®é¢˜æ£€ç´¢æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
            return content
        
        print("âŒ æ‰€æœ‰æ£€ç´¢æ–¹æ³•éƒ½å¤±è´¥")
        return ""
                
    except Exception as e:
        print(f"âŒ æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return ""
